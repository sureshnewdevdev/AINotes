<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Vector Database Tutorial Notes</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 20px;
            background-color: #f4f4f4;
            color: #333;
        }
        .container {
            max-width: 800px;
            margin: 0 auto;
            background: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 0 10px rgba(0,0,0,0.1);
        }
        h1, h2, h3 {
            color: #2c3e50;
        }
        code {
            background-color: #f8f9fa;
            padding: 2px 4px;
            border-radius: 4px;
            font-family: monospace;
        }
        pre {
            background-color: #f8f9fa;
            padding: 10px;
            border-left: 4px solid #007bff;
            overflow-x: auto;
        }
        ul, ol {
            margin-left: 20px;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 10px 0;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
        }
        th {
            background-color: #007bff;
            color: white;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Vector Database Tutorial Notes</h1>
        <p><em>Notes on Vector Databases (Vector DBs) for AI/ML applications. Last updated: November 13, 2025.</em></p>

        <h2>1. Introduction to Vector Databases</h2>
        <p>Vector databases are specialized databases designed to store, index, and query high-dimensional vectors (embeddings). These vectors represent data in a numerical format, often generated by machine learning models (e.g., word embeddings from BERT or image embeddings from CLIP).</p>
        <ul>
            <li><strong>Key Use Cases:</strong> Semantic search, recommendation systems, anomaly detection, RAG (Retrieval-Augmented Generation) in LLMs.</li>
            <li><strong>Why Vectors?</strong> Traditional DBs (SQL/NoSQL) handle structured data well, but vectors enable similarity-based queries (e.g., "find similar images").</li>
        </ul>

        <h2>2. How Vector DBs Work</h2>
        <p>Vector DBs use <strong>Approximate Nearest Neighbor (ANN)</strong> algorithms for efficient querying, as exact searches in high dimensions (curse of dimensionality) are computationally expensive.</p>
        
        <h3>Core Components:</h3>
        <ol>
            <li><strong>Vector Embeddings:</strong> Convert data (text, images) into dense vectors using models like OpenAI's embeddings API.</li>
            <li><strong>Indexing:</strong> Structures like HNSW (Hierarchical Navigable Small World), IVF (Inverted File), or PQ (Product Quantization) to speed up searches.</li>
            <li><strong>Querying:</strong> Compute distance metrics (e.g., cosine similarity, Euclidean distance) to find top-k similar vectors.</li>
            <li><strong>Metadata Storage:</strong> Often store additional info (e.g., original text) alongside vectors.</li>
        </ol>

        <h3>Distance Metrics:</h3>
        <table>
            <thead>
                <tr>
                    <th>Metric</th>
                    <th>Description</th>
                    <th>Use Case</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Cosine Similarity</td>
                    <td>Angle between vectors (ignores magnitude)</td>
                    <td>Text similarity</td>
                </tr>
                <tr>
                    <td>Euclidean Distance</td>
                    <td>Straight-line distance</td>
                    <td>Image features</td>
                </tr>
                <tr>
                    <td>Manhattan Distance</td>
                    <td>Sum of absolute differences</td>
                    <td>Sparse data</td>
                </tr>
            </tbody>
        </table>

        <h2>3. Popular Vector Databases</h2>
        <ul>
            <li><strong>Pinecone:</strong> Managed cloud service, easy API, scales well.</li>
            <li><strong>Milvus:</strong> Open-source, supports multiple indexes, good for large-scale.</li>
            <li><strong>Weaviate:</strong> GraphQL API, integrates with ML frameworks.</li>
            <li><strong>FAISS (Facebook AI Similarity Search):</strong> Library for building indexes, not a full DB.</li>
            <li><strong>Chroma:</strong> Lightweight, embeddable for local use.</li>
        </ul>

        <h2>4. Simple Tutorial: Using Chroma (Python Example)</h2>
        <p>Install Chroma: <code>pip install chromadb</code></p>
        <p>Basic workflow:</p>
        
        <pre><code>import chromadb
from chromadb.utils import embedding_functions

# Initialize client
client = chromadb.Client()

# Create collection with OpenAI embeddings (replace with your API key)
ef = embedding_functions.OpenAIEmbeddingFunction(api_key="your-openai-key")
collection = client.create_collection(name="tutorial", embedding_function=ef)

# Add documents (auto-embeds)
collection.add(
    documents=["The quick brown fox jumps over the lazy dog.", "A cat sat on the mat."],
    ids=["doc1", "doc2"]
)

# Query for similar docs
results = collection.query(
    query_texts=["What about a fox and a dog?"],
    n_results=2
)
print(results)</code></pre>

        <p><strong>Output:</strong> Returns IDs, distances, and metadata of top similar docs.</p>

        <h2>5. Pros and Cons</h2>
        <table>
            <thead>
                <tr>
                    <th>Pros</th>
                    <th>Cons</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Fast similarity searches</td>
                    <td>Approximate results (not exact)</td>
                </tr>
                <tr>
                    <td>Handles unstructured data</td>
                    <td>High memory usage for large datasets</td>
                </tr>
                <tr>
                    <td>Scalable for AI apps</td>
                    <td>Learning curve for indexing</td>
                </tr>
            </tbody>
        </table>

        <h2>6. Best Practices & Next Steps</h2>
        <ul>
            <li><strong>Normalize Vectors:</strong> Scale to unit length for cosine similarity.</li>
            <li><strong>Hybrid Search:</strong> Combine with keyword search for better recall.</li>
            <li><strong>Monitor Performance:</strong> Tune index parameters based on query load.</li>
            <li><strong>Next:</strong> Explore RAG pipelines with LangChain + Vector DBs.</li>
        </ul>

        <p><em>Resources: Official docs for Chroma/Pinecone; Papers on HNSW/ANN algorithms.</em></p>
    </div>
</body>
</html>